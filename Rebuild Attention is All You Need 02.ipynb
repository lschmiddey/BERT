{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from boltons.iterutils import windowed\n",
    "\n",
    "# Helper libraries\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torchtext\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'Data/Market_Basket_Optimisation.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('data/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('data/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val): \n",
    "    for key, value in item2idx.items(): \n",
    "         if val == value:\n",
    "                return key \n",
    "  \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(keyval): \n",
    "    for key, value in item2idx.items(): \n",
    "         if keyval == key:\n",
    "                return value \n",
    "  \n",
    "    return \"value doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, sequence_length=4):\n",
    "    df = pd.read_csv(DATA_PATH, sep=';', names=['sequence'])\n",
    "    df = df.sequence.str.lower().tolist()\n",
    "    tmp = []\n",
    "    for row in df:\n",
    "        splitted = ((row.split(',')))\n",
    "        tmp.append(windowed(splitted, sequence_length))\n",
    "    all_chars_windowed = [sublst for lst in tmp for sublst in lst]\n",
    "    filtered_good_chars = [\n",
    "            sequence for sequence in tqdm_notebook(all_chars_windowed) \n",
    "        ]\n",
    "    return filtered_good_chars\n",
    "\n",
    "\n",
    "def get_unique_items(sequences):\n",
    "    return {sublst for lst in sequences for sublst in lst}\n",
    "\n",
    "\n",
    "def create_item2idx(sequences):\n",
    "    unique_chars = get_unique_items(sequences)\n",
    "    return {char: idx for idx, char in enumerate(sorted(unique_chars))}\n",
    "\n",
    "\n",
    "def encode_sequence(sequence, item2idx):\n",
    "    return [item2idx[char] for char in sequence]\n",
    "\n",
    "\n",
    "def encode_sequences(sequences, char2idx):\n",
    "    return np.array([\n",
    "        encode_sequence(sequence, char2idx) \n",
    "        for sequence in tqdm_notebook(sequences)\n",
    "    ])\n",
    "\n",
    "def split_list_x_and_y(encoded_seq):\n",
    "    # our y value should be the last value of each sequence ->\n",
    "    # IMPORTANT: depends on sequence length!\n",
    "    y_num = encoded_seq[:,2:4]\n",
    "    x_num = encoded_seq[:,0:2]\n",
    "    return x_num, y_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell soll innerhalb einer Session den jeweiligen state feststellen, daher muss das Validation Set in der gleichen Session liegen wie die Trainsession; allerdings zu einem (im Idealfall) sp√§teren/letzten Zustand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequences(Dataset):\n",
    "    def __init__(self, path, trainset=True):\n",
    "        \n",
    "        self.sequences = load_data(path)\n",
    "        self.vocab_size = len(get_unique_items(self.sequences))\n",
    "        self.item2idx = create_item2idx(self.sequences)\n",
    "        self.idx2item = {idx: item for item, idx in self.item2idx.items()}\n",
    "        self.x, self.y = split_list_x_and_y(encode_sequences(self.sequences, self.item2idx))\n",
    "        self.dataseq_len = self.x.shape[0]\n",
    "        # to build training and testset, we cannot simply use train-test split, because sequences are not random but in order\n",
    "        self.train_len = np.ceil(self.dataseq_len*0.8).astype(int)\n",
    "        \n",
    "        if trainset==True:\n",
    "            self.x, self.y = self.x[0:self.train_len], self.y[0:self.train_len]\n",
    "        else:\n",
    "            self.x, self.y = self.x[self.train_len:], self.y[self.train_len:]\n",
    "            \n",
    "        self.x = torch.from_numpy(self.x).to(device)\n",
    "        self.y = torch.from_numpy(self.y).to(device)\n",
    "#         self.emb_dims = [(self.vocab_size, min(50, (self.vocab_size + 1) // 2))]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e05f4f927b4d9eb9c9ca58df9d89f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11726), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3363990723dd434ebd7981909f87d55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11726), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00466d41a3914f2ca5808140d0db0487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11726), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3658550499483cb9cad6b917b09b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11726), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_emb_train = Sequences(DATA_PATH, trainset=True)\n",
    "dataset_emb_test = Sequences(DATA_PATH, trainset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9381, 2]), torch.Size([9381, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_emb_train.x.shape, dataset_emb_train.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_emb_train=DataLoader(dataset=dataset_emb_train,batch_size=64, shuffle=False)\n",
    "dataloader_emb_test=DataLoader(dataset=dataset_emb_test,batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 120)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_emb_train.vocab_size, dataset_emb_test.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9381, 2]), torch.Size([2345, 2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_emb_train.dataset.x.shape, dataloader_emb_test.dataset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dataloader_emb_test.dataset.idx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 71, 111], dtype=torch.int32), tensor([111,  37], dtype=torch.int32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_emb_test.dataset.x[17], dataloader_emb_test.dataset.x[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([37, 25], dtype=torch.int32), tensor([25, 43], dtype=torch.int32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_emb_test.dataset.y[17], dataloader_emb_test.dataset.y[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('milk', 'vegetables mix', 'eggs')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item2idx = dataloader_emb_train.dataset.item2idx\n",
    "get_key(71), get_key(111), get_key(37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, src len]\n",
    "                \n",
    "        #self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiheadAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, seq len, seq len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "                \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        #x = [batch size, n heads, seq len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #x = [batch size, seq len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "                            \n",
    "        #pos = [batch size, trg len]\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "                \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "            \n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        \n",
    "        #trg_pad_mask = [batch size, 1, trg len, 1]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = dataset_emb_train.vocab_size\n",
    "OUTPUT_DIM = dataset_emb_train.vocab_size\n",
    "#HID_DIM = 256\n",
    "# die hidden_dim geben auch die embedding-size vor, wenn es pre-trained embeddings gibt, m√ºssen die HID_DIM angepasst werden\n",
    "HID_DIM = 128\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 100\n",
    "DEC_PF_DIM = 100\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = max(dataset_emb_train.idx2item)+1\n",
    "TRG_PAD_IDX = max(dataset_emb_train.idx2item)+1\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(120, 128)\n",
       "    (pos_embedding): Embedding(5, 128)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(120, 128)\n",
       "    (pos_embedding): Embedding(5, 128)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=128, out_features=120, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 798,416 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "            \n",
    "    for batch_idx, (inputs,outputs) in enumerate(iterator):\n",
    "        src = inputs.long().to(device)\n",
    "        trg = outputs.long().to(device)\n",
    "        batch_size = src.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, (inputs,outputs) in enumerate(iterator):\n",
    "            src = inputs.long().to(device)\n",
    "            trg = outputs.long().to(device)\n",
    "            batch_size = src.size(0)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 20s\n",
      "\tTrain Loss: 2.489 | Train PPL:  12.043\n",
      "Epoch: 02 | Time: 0m 23s\n",
      "\tTrain Loss: 2.504 | Train PPL:  12.230\n",
      "Epoch: 03 | Time: 0m 24s\n",
      "\tTrain Loss: 2.474 | Train PPL:  11.872\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 3\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, dataloader_emb_train, optimizer, criterion, CLIP)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    " \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Problem hier beim Validation Set ist, dass die Sessions per se keine inh√§rente Logik haben wir ein Satz. Je genauer wir die Trainingssessions lernen, desto weiter sind wir vom Validationset entfernt, weil wir uns immer \"sicherer\" werden. Wenn die Sequenz aber nicht 100%ig stimmt, verschlechtern wir den Val.Loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lasse\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\lasse\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\lasse\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\lasse\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type MultiHeadAttentionLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\lasse\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedforwardLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\lasse\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\lasse\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "file_name = \"Attention_Rebuild02.pth\"\n",
    "torch.save(model, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embedding model\n",
    "file_name = \"Attention_Rebuild02.pth\"\n",
    "model_embeddings = torch.load(file_name, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(120, 128)\n",
       "    (pos_embedding): Embedding(5, 128)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(120, 128)\n",
       "    (pos_embedding): Embedding(5, 128)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=128, out_features=100, bias=True)\n",
       "          (fc_2): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=128, out_features=120, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Next Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier noch mit target_seq (k√∂nnte man auch theoretisch nehmen, wenn man 10 Sequenzen abwartet und dann 5 als input und die n√§chsten 5 als Output, allerdings dann nicht mehr wirklich korrekt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k√ºnstliche Sequenz der L√§nge n erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem ist noch bei BERT: die Input Query wird gegen die Output Query geworfen, um eine weitere Sequenz zu erzeugen. Was ich versuchen m√∂chte, ist, den Input selbst als Query gegen den Session-state zu werfen und daraus selbst eine neue Sequenz zu erzeugen. Bzw sollte ich als Input n eingeben, als target n+1, also einfach einen weiter. Der aktuelle State ist der Target state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au√üerdem muss die Reihenfolge sein: neuester angesehener Artikel bis vor n- Klicks angesehener Artikel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq(src_tensor, trg_tensor, n=5):\n",
    "    for i in range (0,n):\n",
    "    \n",
    "        if i ==0:\n",
    "            src_tensor_input = src_tensor\n",
    "            trg_tensor_input = trg_tensor\n",
    "        else:\n",
    "            src_tensor_input = src_tensor_\n",
    "            trg_tensor_input = trg_tensor_\n",
    "\n",
    "        src_mask = model_embeddings.make_src_mask(src_tensor_input)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            enc_src = model_embeddings.encoder(src_tensor_input, src_mask)\n",
    "\n",
    "        trg_mask = model_embeddings.make_trg_mask(trg_tensor_input)\n",
    "\n",
    "        with torch.no_grad():\n",
    "                output, attention = model_embeddings.decoder(trg_tensor_input, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # # den wahrscheinlichsten Output nehmen        \n",
    "        pred_token = output.argmax(2)[:,0].item()\n",
    "\n",
    "        # # von der alten Sequenz den am weitesten entfernt gesehenen l√∂schen und pred anh√§ngen\n",
    "        src_tensor_ = torch.cat((torch.tensor(pred_token).detach().view(-1).to(device), src_tensor_input[0][:-1]), dim=0).unsqueeze(0)\n",
    "        trg_tensor_ = torch.cat((torch.tensor(pred_token).detach().view(-1).to(device), trg_tensor_input[0][:-1]), dim=0).unsqueeze(0)\n",
    "        \n",
    "        if i == 0:\n",
    "            complete_gen_seq = torch.cat((torch.tensor(pred_token).detach().view(-1).to(device), src_tensor_input[0]), dim=0).unsqueeze(0)\n",
    "        elif i == n-1:\n",
    "            complete_gen_seq = torch.cat((torch.tensor(pred_token).detach().view(-1).to(device), complete_gen_seq[0]), dim=0).unsqueeze(0)\n",
    "            # nur f√ºr die sp√§tere Ausgabe soll zuerst der Artikel angezeigt werden, der vor n Klicks gesehen wurde, \n",
    "            # und zuletzt der gerade gesehene Artikel (bzw. der von BERT vorhergesagte Artikel)\n",
    "            complete_gen_seq = torch.flip(complete_gen_seq, [0,1])\n",
    "        else:\n",
    "            complete_gen_seq = torch.cat((torch.tensor(pred_token).detach().view(-1).to(device), complete_gen_seq[0]), dim=0).unsqueeze(0)\n",
    "               \n",
    "    return src_tensor_, complete_gen_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[62,  7]]), tensor([[29, 62]]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_no = 8\n",
    "src_tensor = dataloader_emb_test.dataset.x[idx_no].long().unsqueeze(0)\n",
    "src_tensor = torch.flip(src_tensor, [0,1])\n",
    "trg_tensor = dataloader_emb_test.dataset.x[idx_no+1].long().unsqueeze(0)\n",
    "trg_tensor = torch.flip(trg_tensor, [0,1])\n",
    "src_tensor, trg_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barbecue sauce\n",
      "ketchup\n",
      "ab jetzt predictions\n",
      "french fries\n",
      "hot dogs\n",
      "green tea\n"
     ]
    }
   ],
   "source": [
    "src_tensor_, complete_gen_seq = gen_seq(src_tensor, trg_tensor, n=3)\n",
    "for i in range(0,len(complete_gen_seq[0])):\n",
    "    if i == 2:\n",
    "        print('ab jetzt predictions')\n",
    "    print(get_key(complete_gen_seq[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selbst Sequenzen erstellen und gucken, wie BERT sie weiterf√ºhren w√ºrde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('avocado', 4)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_emb_train.idx2item[4], dataset_emb_train.item2idx['avocado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' asparagus',\n",
       " 1: 'almonds',\n",
       " 2: 'antioxydant juice',\n",
       " 3: 'asparagus',\n",
       " 4: 'avocado',\n",
       " 5: 'babies food',\n",
       " 6: 'bacon',\n",
       " 7: 'barbecue sauce',\n",
       " 8: 'black tea',\n",
       " 9: 'blueberries',\n",
       " 10: 'body spray',\n",
       " 11: 'bramble',\n",
       " 12: 'brownies',\n",
       " 13: 'bug spray',\n",
       " 14: 'burger sauce',\n",
       " 15: 'burgers',\n",
       " 16: 'butter',\n",
       " 17: 'cake',\n",
       " 18: 'candy bars',\n",
       " 19: 'carrots',\n",
       " 20: 'cauliflower',\n",
       " 21: 'cereals',\n",
       " 22: 'champagne',\n",
       " 23: 'chicken',\n",
       " 24: 'chili',\n",
       " 25: 'chocolate',\n",
       " 26: 'chocolate bread',\n",
       " 27: 'chutney',\n",
       " 28: 'cider',\n",
       " 29: 'clothes accessories',\n",
       " 30: 'cookies',\n",
       " 31: 'cooking oil',\n",
       " 32: 'corn',\n",
       " 33: 'cottage cheese',\n",
       " 34: 'cream',\n",
       " 35: 'dessert wine',\n",
       " 36: 'eggplant',\n",
       " 37: 'eggs',\n",
       " 38: 'energy bar',\n",
       " 39: 'energy drink',\n",
       " 40: 'escalope',\n",
       " 41: 'extra dark chocolate',\n",
       " 42: 'flax seed',\n",
       " 43: 'french fries',\n",
       " 44: 'french wine',\n",
       " 45: 'fresh bread',\n",
       " 46: 'fresh tuna',\n",
       " 47: 'fromage blanc',\n",
       " 48: 'frozen smoothie',\n",
       " 49: 'frozen vegetables',\n",
       " 50: 'gluten free bar',\n",
       " 51: 'grated cheese',\n",
       " 52: 'green beans',\n",
       " 53: 'green grapes',\n",
       " 54: 'green tea',\n",
       " 55: 'ground beef',\n",
       " 56: 'gums',\n",
       " 57: 'ham',\n",
       " 58: 'hand protein bar',\n",
       " 59: 'herb & pepper',\n",
       " 60: 'honey',\n",
       " 61: 'hot dogs',\n",
       " 62: 'ketchup',\n",
       " 63: 'light cream',\n",
       " 64: 'light mayo',\n",
       " 65: 'low fat yogurt',\n",
       " 66: 'magazines',\n",
       " 67: 'mashed potato',\n",
       " 68: 'mayonnaise',\n",
       " 69: 'meatballs',\n",
       " 70: 'melons',\n",
       " 71: 'milk',\n",
       " 72: 'mineral water',\n",
       " 73: 'mint',\n",
       " 74: 'mint green tea',\n",
       " 75: 'muffins',\n",
       " 76: 'mushroom cream sauce',\n",
       " 77: 'napkins',\n",
       " 78: 'nonfat milk',\n",
       " 79: 'oatmeal',\n",
       " 80: 'oil',\n",
       " 81: 'olive oil',\n",
       " 82: 'pancakes',\n",
       " 83: 'parmesan cheese',\n",
       " 84: 'pasta',\n",
       " 85: 'pepper',\n",
       " 86: 'pet food',\n",
       " 87: 'pickles',\n",
       " 88: 'protein bar',\n",
       " 89: 'red wine',\n",
       " 90: 'rice',\n",
       " 91: 'salad',\n",
       " 92: 'salmon',\n",
       " 93: 'salt',\n",
       " 94: 'sandwich',\n",
       " 95: 'shallot',\n",
       " 96: 'shampoo',\n",
       " 97: 'shrimp',\n",
       " 98: 'soda',\n",
       " 99: 'soup',\n",
       " 100: 'spaghetti',\n",
       " 101: 'sparkling water',\n",
       " 102: 'spinach',\n",
       " 103: 'strawberries',\n",
       " 104: 'strong cheese',\n",
       " 105: 'tea',\n",
       " 106: 'tomato juice',\n",
       " 107: 'tomato sauce',\n",
       " 108: 'tomatoes',\n",
       " 109: 'toothpaste',\n",
       " 110: 'turkey',\n",
       " 111: 'vegetables mix',\n",
       " 112: 'water spray',\n",
       " 113: 'white wine',\n",
       " 114: 'whole weat flour',\n",
       " 115: 'whole wheat pasta',\n",
       " 116: 'whole wheat rice',\n",
       " 117: 'yams',\n",
       " 118: 'yogurt cake',\n",
       " 119: 'zucchini'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_emb_train.idx2item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 59, 81, 83)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_val('eggplant'), get_val('herb & pepper'),get_val('olive oil'),get_val('parmesan cheese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darauf achten, dass die Reihenfolge ist vom gerade gesehenen zum vor n gesehenen Artikel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tensor_created = torch.tensor([81, 83]).unsqueeze(0).to(device)\n",
    "# trg_tensor ist src_tensor plus einen Artikel und nach hinten verschoben\n",
    "trg_tensor_created = torch.tensor([59, 81]).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was sich erkennen l√§sst: wenn der gerade gesehene Artikel (also der Artikel, der in trg_tensor aber nicht in src_tensor drin ist) anders ist, als der zuletzt gesehene Artikel, dann werden die vorgeschlagenen n√§chsten Artikel anders, eher dazu passend. Wenn es sich um einen √§hnlichen Artikel handelt, dann werden auch √§hnliche Artikel als folgende Sequenz generiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parmesan cheese\n",
      "olive oil\n",
      "ab jetzt predictions\n",
      "ground beef\n",
      "spaghetti\n",
      "mineral water\n"
     ]
    }
   ],
   "source": [
    "src_tensor_, complete_gen_seq = gen_seq(src_tensor_created, trg_tensor_created, n=3)\n",
    "for i in range(0,len(complete_gen_seq[0])):\n",
    "    if i == 2:\n",
    "        print('ab jetzt predictions')\n",
    "    print(get_key(complete_gen_seq[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Seq Generation mit topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq_topk(src_tensor, trg_tensor, n=5, topk=5):\n",
    "    \n",
    "    model_embeddings.eval()\n",
    "    \n",
    "    for i in range (0,n):\n",
    "        \n",
    "        # in der ersten Iteration wird der Ausgangs src und trg Tensor eingegeben, daraufhin werden src und trg Tensor jeweils\n",
    "        # von den k√ºnstlich erzeugten Artikeln √ºberschrieben\n",
    "        if i ==0:\n",
    "            src_tensor_input = src_tensor\n",
    "            trg_tensor_input = trg_tensor\n",
    "        else:\n",
    "            src_tensor_input = src_tensor_\n",
    "            trg_tensor_input = trg_tensor_\n",
    "\n",
    "        src_mask = model_embeddings.make_src_mask(src_tensor_input)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            enc_src = model_embeddings.encoder(src_tensor_input, src_mask)\n",
    "\n",
    "        trg_mask = model_embeddings.make_trg_mask(trg_tensor_input)\n",
    "\n",
    "        with torch.no_grad():\n",
    "                output, attention = model_embeddings.decoder(trg_tensor_input, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # einen Zufallsoutput aus den Topk nehmen \n",
    "        rnd_nmbr = np.random.randint(topk)\n",
    "        top_values, top_art = torch.topk(output[0][0],topk)\n",
    "        pred_token = top_art[rnd_nmbr]\n",
    "\n",
    "        # # von der alten Sequenz den am weitesten entfernt gesehenen l√∂schen und pred anh√§ngen\n",
    "        src_tensor_ = torch.cat((torch.clone(pred_token).detach().view(-1).to(device), src_tensor_input[0][:-1]), dim=0).unsqueeze(0)\n",
    "        trg_tensor_ = torch.cat((torch.clone(pred_token).detach().view(-1).to(device), trg_tensor_input[0][:-1]), dim=0).unsqueeze(0)\n",
    "        \n",
    "        if i == 0:\n",
    "            complete_gen_seq = torch.cat((torch.clone(pred_token).detach().view(-1).to(device), src_tensor_input[0]), dim=0).unsqueeze(0)\n",
    "        elif i == n-1:\n",
    "            complete_gen_seq = torch.cat((torch.clone(pred_token).detach().view(-1).to(device), complete_gen_seq[0]), dim=0).unsqueeze(0)\n",
    "            # nur f√ºr die sp√§tere Ausgabe soll zuerst der Artikel angezeigt werden, der vor n Klicks gesehen wurde, \n",
    "            # und zuletzt der gerade gesehene Artikel (bzw. der von BERT vorhergesagte Artikel)\n",
    "            complete_gen_seq = torch.flip(complete_gen_seq, [0,1])\n",
    "        else:\n",
    "            complete_gen_seq = torch.cat((torch.clone(pred_token).detach().view(-1).to(device), complete_gen_seq[0]), dim=0).unsqueeze(0)\n",
    "            \n",
    "    return src_tensor_, complete_gen_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[72, 55]]), tensor([[90, 72]]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_no = 5\n",
    "src_tensor = dataloader_emb_test.dataset.x[idx_no].long().unsqueeze(0)\n",
    "src_tensor = torch.flip(src_tensor, [0,1])\n",
    "trg_tensor = dataloader_emb_test.dataset.x[idx_no+1].long().unsqueeze(0)\n",
    "trg_tensor = torch.flip(trg_tensor, [0,1])\n",
    "src_tensor, trg_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rice'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_key(trg_tensor[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground beef\n",
      "mineral water\n",
      "ab jetzt predictions\n",
      "protein bar\n",
      "toothpaste\n",
      "candy bars\n"
     ]
    }
   ],
   "source": [
    "src_tensor_, complete_gen_seq = gen_seq_topk(src_tensor, trg_tensor, n=3, topk=5)\n",
    "for i in range(0,len(complete_gen_seq[0])):\n",
    "    if i == 2:\n",
    "        print('ab jetzt predictions')\n",
    "    print(get_key(complete_gen_seq[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  selbst Sequenzen erstellen und gegen topk testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' asparagus',\n",
       " 1: 'almonds',\n",
       " 2: 'antioxydant juice',\n",
       " 3: 'asparagus',\n",
       " 4: 'avocado',\n",
       " 5: 'babies food',\n",
       " 6: 'bacon',\n",
       " 7: 'barbecue sauce',\n",
       " 8: 'black tea',\n",
       " 9: 'blueberries',\n",
       " 10: 'body spray',\n",
       " 11: 'bramble',\n",
       " 12: 'brownies',\n",
       " 13: 'bug spray',\n",
       " 14: 'burger sauce',\n",
       " 15: 'burgers',\n",
       " 16: 'butter',\n",
       " 17: 'cake',\n",
       " 18: 'candy bars',\n",
       " 19: 'carrots',\n",
       " 20: 'cauliflower',\n",
       " 21: 'cereals',\n",
       " 22: 'champagne',\n",
       " 23: 'chicken',\n",
       " 24: 'chili',\n",
       " 25: 'chocolate',\n",
       " 26: 'chocolate bread',\n",
       " 27: 'chutney',\n",
       " 28: 'cider',\n",
       " 29: 'clothes accessories',\n",
       " 30: 'cookies',\n",
       " 31: 'cooking oil',\n",
       " 32: 'corn',\n",
       " 33: 'cottage cheese',\n",
       " 34: 'cream',\n",
       " 35: 'dessert wine',\n",
       " 36: 'eggplant',\n",
       " 37: 'eggs',\n",
       " 38: 'energy bar',\n",
       " 39: 'energy drink',\n",
       " 40: 'escalope',\n",
       " 41: 'extra dark chocolate',\n",
       " 42: 'flax seed',\n",
       " 43: 'french fries',\n",
       " 44: 'french wine',\n",
       " 45: 'fresh bread',\n",
       " 46: 'fresh tuna',\n",
       " 47: 'fromage blanc',\n",
       " 48: 'frozen smoothie',\n",
       " 49: 'frozen vegetables',\n",
       " 50: 'gluten free bar',\n",
       " 51: 'grated cheese',\n",
       " 52: 'green beans',\n",
       " 53: 'green grapes',\n",
       " 54: 'green tea',\n",
       " 55: 'ground beef',\n",
       " 56: 'gums',\n",
       " 57: 'ham',\n",
       " 58: 'hand protein bar',\n",
       " 59: 'herb & pepper',\n",
       " 60: 'honey',\n",
       " 61: 'hot dogs',\n",
       " 62: 'ketchup',\n",
       " 63: 'light cream',\n",
       " 64: 'light mayo',\n",
       " 65: 'low fat yogurt',\n",
       " 66: 'magazines',\n",
       " 67: 'mashed potato',\n",
       " 68: 'mayonnaise',\n",
       " 69: 'meatballs',\n",
       " 70: 'melons',\n",
       " 71: 'milk',\n",
       " 72: 'mineral water',\n",
       " 73: 'mint',\n",
       " 74: 'mint green tea',\n",
       " 75: 'muffins',\n",
       " 76: 'mushroom cream sauce',\n",
       " 77: 'napkins',\n",
       " 78: 'nonfat milk',\n",
       " 79: 'oatmeal',\n",
       " 80: 'oil',\n",
       " 81: 'olive oil',\n",
       " 82: 'pancakes',\n",
       " 83: 'parmesan cheese',\n",
       " 84: 'pasta',\n",
       " 85: 'pepper',\n",
       " 86: 'pet food',\n",
       " 87: 'pickles',\n",
       " 88: 'protein bar',\n",
       " 89: 'red wine',\n",
       " 90: 'rice',\n",
       " 91: 'salad',\n",
       " 92: 'salmon',\n",
       " 93: 'salt',\n",
       " 94: 'sandwich',\n",
       " 95: 'shallot',\n",
       " 96: 'shampoo',\n",
       " 97: 'shrimp',\n",
       " 98: 'soda',\n",
       " 99: 'soup',\n",
       " 100: 'spaghetti',\n",
       " 101: 'sparkling water',\n",
       " 102: 'spinach',\n",
       " 103: 'strawberries',\n",
       " 104: 'strong cheese',\n",
       " 105: 'tea',\n",
       " 106: 'tomato juice',\n",
       " 107: 'tomato sauce',\n",
       " 108: 'tomatoes',\n",
       " 109: 'toothpaste',\n",
       " 110: 'turkey',\n",
       " 111: 'vegetables mix',\n",
       " 112: 'water spray',\n",
       " 113: 'white wine',\n",
       " 114: 'whole weat flour',\n",
       " 115: 'whole wheat pasta',\n",
       " 116: 'whole wheat rice',\n",
       " 117: 'yams',\n",
       " 118: 'yogurt cake',\n",
       " 119: 'zucchini'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_emb_train.idx2item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 78, 72)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_val('protein bar'), get_val('nonfat milk'),get_val('mineral water')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tensor_created = torch.tensor([88, 72]).unsqueeze(0).to(device)\n",
    "# trg_tensor ist src_tensor plus einen Artikel und nach hinten verschoben\n",
    "trg_tensor_created = torch.tensor([63, 88]).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mineral water\n",
      "protein bar\n",
      "ab jetzt predictions\n",
      "cottage cheese\n",
      "honey\n",
      "champagne\n",
      "body spray\n",
      "salt\n"
     ]
    }
   ],
   "source": [
    "src_tensor_, complete_gen_seq = gen_seq_topk(src_tensor_created, trg_tensor_created, n=5, topk=25)\n",
    "for i in range(0,len(complete_gen_seq[0])):\n",
    "    if i == 2:\n",
    "        print('ab jetzt predictions')\n",
    "    print(get_key(complete_gen_seq[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
